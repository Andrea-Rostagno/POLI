{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "macro-marathon",
   "metadata": {},
   "source": [
    "# Python Lab 05b: Ricerca degli Iper-parametri\n",
    "## Francesco Della Santa, Matematica per l'Intelligenza Artificiale, Politecnico di Torino\n",
    "\n",
    "Come visto nelle scorse esercitazioni, diversi valori/scelte di iper-parametri possono dare risultati notevolmente differenti. Diventa quindi importante determinare quali di essi costituiscano la miglior combinazione rispetto ai risultati desiderati per il modello di apprendimento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-karen",
   "metadata": {},
   "source": [
    "## Richiamo: Training Set, Validation Set e Test Set\n",
    "\n",
    "Dato un dataset di coppie features-target \n",
    "$$\\mathcal{D} = \\{(\\boldsymbol{x}_1, \\boldsymbol{y}_1),\\ldots ,(\\boldsymbol{x}_D, \\boldsymbol{y}_D)\\}\\subset\\mathbb{R}^n\\times\\mathbb{R}^m\\,,$$\n",
    "\n",
    "per esempio per un problema di classificazione in $m$ classi, per addestrare un modello di Machine Learning (ML) generalmente si divide $\\mathcal{D}$ in un _training set_ $\\mathcal{T}$ ed un _test set_ $\\mathcal{P}$ tali che:\n",
    "1. le coppie $(\\boldsymbol{x}, \\boldsymbol{y})$ in $\\mathcal{T}$ vengono utilizzate per addestrare il modello ed \"insegnargli\" l'operazione desiderata (p.e., la classificazione rispetto $m$ classi);\n",
    "2. le coppie $(\\boldsymbol{x}, \\boldsymbol{y})$ in $\\mathcal{P}$ vengono utilizzate per _quantificare_ quanto bene un modello addestrato abbia imparato l'operazione desiderata, rispetto un'arbitraria funzione di valutazione.\n",
    "\n",
    "**RICORDA:** ovviamente $\\mathcal{P}$ deve essere utilizato _solo ed esclusivamente per la valutazione delle performance_! Ogni suo coinvolgimento nelle operazioni di addestramento renderebbe meno affidabili le performance misurate su di esso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-directory",
   "metadata": {},
   "source": [
    "### Il Validation Set\n",
    "\n",
    "In molti casi, è tuttavia utile disporre di una previsione delle possibili performance su $\\mathcal{P}$ per un modello, per esempio quando:\n",
    "1. si deve eseguire una ricerca degli iper-parametri ottimali per il problema;\n",
    "2. si deve monitorare l'andamento di un addestramento caratterizzato da un processo iterativo;\n",
    "3. ecc.\n",
    "\n",
    "Nei casi sopra citati, risulta quindi utile dividere $\\mathcal{D}$ non in due, ma in tre sottoinsiemi: training set $\\mathcal{T}$, *validation set* $\\mathcal{V}$ e test set $\\mathcal{P}$. \n",
    "Mentre $\\mathcal{T}$ e $\\mathcal{P}$ svolgono i soliti ruoli, il validation set $\\mathcal{V}$ funge da \"pre-test set\", cioè viene utilizzato per svolgere le operazioni sopra citate, generalmente misurando le performance di modelli addestrati su $\\mathcal{T}$ per avere una (sovra)stima delle possibili performance \"finali\" su $\\mathcal{P}$.\n",
    "\n",
    "Concentriamoci sul punto (1) dell'elenco sovrastante e assumiamo di avere $K$ modelli $\\hat{f}_1,\\ldots ,\\hat{f}_K$ caratterizzati da $K$ diverse combinazioni di iper-parametri. In poche parole, il procedimento di ricerca del miglior modello rispetto $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$ è il seguente:\n",
    "1. addestro ogni modello $\\hat{f}_1,\\ldots ,\\hat{f}_K$ su $\\mathcal{T}$;\n",
    "2. valuto su $\\mathcal{V}$ le performance dei modelli addestrati $\\hat{f}_1,\\ldots ,\\hat{f}_K$ ed indentifico il migliore;\n",
    "3. misuro le performance del modello migliore su $\\mathcal{P}$ per avere una stima delle sue performance in generale nel futuro.\n",
    "\n",
    "**NOTA BENE:** nella pratica, il validation set è sempre quello con cardinalità minore, cioè: $|\\mathcal{V}|<|\\mathcal{T}|,|\\mathcal{P}|$; per questo motivo le performance su $\\mathcal{V}$ sono generalmente una sovrastima di quelle su $\\mathcal{P}$.\n",
    "\n",
    "**ATTENZIONE:** in letteratura spesso i termini _validation set_ e _test set_ hanno un significato molto \"fluido\". La definizione e l'utilizzo sopra descritti sono la versione più comune ed utilizzata in ambito ML; tuttavia, si posono incontrare alcune altre convenzioni:\n",
    "1. in ambito di ML, può capitare di sentir parlare solamente di training e validation set. In questo caso, il validation set svolge il ruolo di quello che noi abbiamo definito test set. Un'estensione di questa convenzione è il caso della $k$_-fold cross-validation_ (non la affronteremo in questa esercitazione);\n",
    "2. in ambito Deep Learning (DL) o reti neurali in generale, il modello viene addestrato rispetto $\\mathcal{T}$ e $\\mathcal{V}$ e valutato su $\\mathcal{P}$. In particolare, il validation set $\\mathcal{V}$ viene utilizzato per \"regolarizzare/gestire\" l'addestramento della rete. Per la ricerca di iper-parametri ottimali, si mettono quindi a confronto le performance su $\\mathcal{P}$ (e non su $\\mathcal{V}$ come indicato sopra). Quindi, se si desidera avere una valutazione oggettiva della rete neurale con la miglior combinazione di iper-parametri, si valutano le performance su un secondo test set $\\mathcal{P}'$ (una sorta di \"test set finale\"), ancora diverso da $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$. Generalmente questa seconda valutazione non viene fatta poiché i dataset $\\mathcal{D}$ (ed i test set $\\mathcal{P}$) usati nel DL sono solitamente molto grandi e quindi le performance su $\\mathcal{P}$ si possono assumere essere praticamente uguali a quelle che si avrebbero su $\\mathcal{P}'$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-representation",
   "metadata": {},
   "source": [
    "## La Ricerca a Griglia\n",
    "\n",
    "Per cercare la miglior combinazione di iper-parametri per un modello, si procede generalmente con una _ricerca a griglia_ (in inglese e in letteratura: \"_grid search_\").\n",
    "\n",
    "Dato cioè un intervallo _discreto_ di valori $I_h$ per ogni iperparametro $p_h$, $h=1,\\ldots , H$, si considera la griglia di punti generata dal prodotto cartesiano degli intervalli, cioè:\n",
    "$$G = I_1 \\times \\cdots \\times I_H\\,.$$\n",
    "\n",
    "Abbiamo quindi che ogni punto di $G$ rappresenta una possibile combinazione di iper-parametri per il modello.\n",
    "Secondo quanto scritto sopra, si considerano quindi i $K=|G|$ modelli caratterizzati dalle $K$ combinazioni di iper-parametri differenti e si cerca quello con le migliori performance su $\\mathcal{V}$.\n",
    "\n",
    "**NOTA BENE:** nella pratica gli intervalli discreti $I_h$ sono raramente equispaziati, preferendo un campionamento casuale determinato da distribuzioni di probabilità."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-straight",
   "metadata": {},
   "source": [
    "## Strumenti di Scikit-Learn per Iperparametri e Performance\n",
    "\n",
    "Per implementare la grid search, nell'esercitazione di oggi, utilizzare il seguente strumento:\n",
    "- GridSearcgCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "- Esempio/guida: https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "**OSSERVAZIONE:** per utilizzi più \"elaborati\", che facciano uso di distribuzioni di probabilità, lo studente può guardare anche altri strumenti, per esempio RandomizedSearchCV (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n",
    "\n",
    "Per implementare il calcolo della matrice di confusione, della precision, della recall e  dell' $F_1$-score, utilizzare i seguenti strumenti:\n",
    "- $F_1$-score: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "- Precision: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "- Recall: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "- make_scorer (generare una delle funzioni sopra, fissando dei parametri): https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
    "- scoring parameters: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "- Confusion Matrix: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "\n",
    "## SVM Nonlineari e Classificazione Non Binaria\n",
    "\n",
    "La classe SVC di sklearn fa uso del metodo One-VS-One (OVO) per la classificazione non binaria (cioè $m>2$ classi).\n",
    "Il metodo consiste nell'addestrare $\\begin{pmatrix}m\\\\ 2\\end{pmatrix} = \\frac{m !}{2!(m-2)!}=\\frac{m(m-1)}{2}$ SVM non lineari per ogni possibile coppia di classi. La classificazione di un vettore $\\boldsymbol{x}$ avviene quindi scegliendo come classe predetta la classe $\\widehat{y}$ che risulta essere predetta dalla maggioranza delle $\\begin{pmatrix}m\\\\ 2\\end{pmatrix}$ SVM.\n",
    "\n",
    "Un'alternativa per la classificazione non binare è lo schema One-VS-Rest (OVR), che addestra $m$ SVM rispetto al problema di classificazione binaria $C_i$ / non-$C_i$, per ogni $i=1,\\ldots ,m$. La classificazione di un vettore $\\boldsymbol{x}$ avviene quindi scegliendo come classe predetta la classe $\\widehat{y}$ che risulta essere predetta tra le varie SVM o, più generalmente e in caso di \"ambiguità\", scegliendo la classe $\\widehat{y}$ predetta con \"maggior sicurezza\" tra le $m$ SVM addestrate.\n",
    "\n",
    "**ATTENZIONE:** La classe SVC di sklearn fa _SEMPRE_ uso del metodo OVO per l'addestramento. Il metodo *decision_function* di default è invece impostato su _'ovr'_ per semplicità di lettura. Specificatamente, anche se le SVM sono addestrate rispetto lo schema OVO, se il metodo *decision_function* è impostato su _'ovr'_ allora esegue una trasformazione monotona sulla classificazione del metodo OVO per restituire una classificazione sullo stile OVR (quindi più semplice da leggere per l'utente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-captain",
   "metadata": {},
   "source": [
    "# Esercitazione: Riconoscimento Volti\n",
    "\n",
    "Nell'esercitazione di oggi, implementeremo una grid-search per SVM nonlineari per la classificazione dei volti già visti nell'esercitazione \"PCAeigenfaces\" e parte del dataset \"Labeled Faces in the Wild\" (LFW).\n",
    "\n",
    "**ATTENZIONE:** Per dettagli sul dataset utilizzato, guardare la vecchia esercitazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** NOTA BENE! *****\n",
    "# perché %matplotlib widget funzioni, installare nell'ambiente virtuale \n",
    "# il pacchetto ipympl con il comando:\n",
    "# pip install ipympl\n",
    "#\n",
    "# ATTENZIONE: perché funzioni è necessario chiudere e rilanciare jupyter-lab\n",
    "#\n",
    "# STILE DI VISUALIZZAZIONE PLOT FATTI CON MATPLOTLIB\n",
    "%matplotlib widget\n",
    "#\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "from IPython.display import display\n",
    "\n",
    "# Il codice presente di seguito serve nel caso si verifichi un errore del tipo\n",
    "#\n",
    "# \"URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1124)>\"\n",
    "#\n",
    "# al momento di chiamare la funzione fetch_lfw_people di sklearn.datasets\n",
    "#\n",
    "# ATTENZIONE: il codice di seguito non è quindi sempre necessario; se non lo fosse, commentarlo pure.\n",
    "#\n",
    "\n",
    "import os, ssl\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-population",
   "metadata": {},
   "source": [
    "## Importazione del Dataset e Creazione di Training, Validation e Test set\n",
    "\n",
    "Importiamo il dataset $\\mathcal{D}$ da scikit-learn e dividiamolo in $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$. Utilizzare le seguenti percentuali:\n",
    "1. $|\\mathcal{T}| = 30\\% |\\mathcal{D}|$\n",
    "1. $|\\mathcal{V}| = 20\\% |\\mathcal{D}|$\n",
    "1. $|\\mathcal{P}| = 50\\% |\\mathcal{D}|$\n",
    "\n",
    "**ATTENZIONE:** visto che andremo ad usare le SVM, _NON_ sarà necessario trasformare le classi secondo la codifica del one-hot encoding.\n",
    "\n",
    "**ATTENZIONE:** per poter utilizzare al meglio la classe di _sklearn_ per la gridsearch, non suddivideremo il dataset direttamente in Training, Validation e Test set, ma \"estrarremo\" gli indici corrispondenti e determinanti tali insiemi.\n",
    "\n",
    "**ESERCIZIO:** completare il codice nella cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "face_data = lfw_people['data']\n",
    "face_images = lfw_people['images']\n",
    "face_tnames = lfw_people['target_names']\n",
    "face_targets = lfw_people['target']\n",
    "\n",
    "# Creare gli indici dei dati corrispondenti a training, validation e test set secondo le percentuali sopra indicate.\n",
    "# Utilizzare i seguenti nomi per le variabili:\n",
    "# Dataset: indices\n",
    "# Training: ind_train\n",
    "# Validation: ind_val\n",
    "# Test: ind_test\n",
    "\n",
    "random_state = 42\n",
    "test_p = 0.5\n",
    "# val_p = ... <-- COMPLETARE!\n",
    "indices = np.arange(face_data.shape[0])\n",
    "\n",
    "# ind_trainval, ind_test = train_test_split(indices, ...) <-- COMPLETARE!\n",
    "# ind_train, ind_val = train_test_split(ind_trainval, ...) <-- COMPLETARE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-potential",
   "metadata": {},
   "source": [
    "## Grid Search ed SVM\n",
    "\n",
    "**ESERCIZIO:** completare il codice nella cella seguente. Impostare una ricerca a griglia per gli iper-parametri delle SVM. In particolare, cercare tra i seguenti valori:\n",
    "1. $C\\in\\{2^i \\ | \\ i=-2, \\ldots , 2\\}$;\n",
    "2. $\\gamma \\in \\{\\frac{1}{i\\cdot n} \\ | \\ i= 0.5, 1, 1.5 \\}$, dove $n$ è il numero di feature del dataset;\n",
    "3. $\\mathrm{kernel} \\in \\{\\mathrm{RBF}, \\mathrm{sigmoid}, \\mathrm{polynomial}, \\mathrm{linear}\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = face_data.shape[1]\n",
    "\n",
    "# Definizione delle liste di valori tra i quali \"scorrere\" per gli iper-parametri:\n",
    "# C_list = ... <-- COMPLETARE!\n",
    "# gamma_list = ... <-- COMPLETARE!\n",
    "# ker_list = ... <-- COMPLETARE!\n",
    "\n",
    "# DIZIONARIO \"SEMPLICE\":\n",
    "# hparameters = {...} <-- COMPLETARE!\n",
    "# ESERCIZIO EXTRA: \n",
    "# DEFINIRE hparameters IN MODO DA EVITARE DI PROVARE kernel='linear' CON PARAMETRI CHE NON LO RIGUARDANO\n",
    "svm = SVC(class_weight='balanced')\n",
    "\n",
    "# svm_gs = GridSearchCV(estimator=..., <-- COMPLETARE! \n",
    "                      # param_grid=..., <-- COMPLETARE! \n",
    "                      # scoring='f1_weighted',\n",
    "                      # return_train_score=True,\n",
    "                      # # refit=True,  <-------------------- SOSTITUIBILE CON FUNZIONI PER PERSONALIZZARE refit, MA INEVITABILE (default True)\n",
    "                      # cv=zip([ind_train], [ind_val]))\n",
    "\n",
    "# OSSERVAZIONE: in alternativa a scoring='f1_weighted', si poteva scrivere equivalentemente\n",
    "# scoring=f1_scorer, dove f1_scorer è una variabile definita tramite make_scorer, cioè:\n",
    "# f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "svm_gs.fit(face_data, face_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostriamo i risultati della Gridsearch con una tabella:\n",
    "\n",
    "df_results = pd.DataFrame(svm_gs.cv_results_)\n",
    "\n",
    "display(df_results.sort_values(['rank_test_score'], ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-ecology",
   "metadata": {},
   "source": [
    "## Performance della Miglior SVM\n",
    "\n",
    "**ESERCIZIO:** calcolare (e visualizzare) di seguito le performance della miglior SVM trovata con la gridsearch, cioè:\n",
    "1. Accuratezza su $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$;\n",
    "2. Precision (average='weighted') su $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$;\n",
    "3. Recall (average='weighted') su $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$;\n",
    "4. $F_1$-score (average='weighted') su $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$;\n",
    "5. Matrice di Confusione su $\\mathcal{P}$ (senza normalizzazione, normalizzata rispetto le vere classi, normalizzara rispetto le classi predette).\n",
    "\n",
    "**ATTENZIONE:** la miglior SVM _deve essere ri-addestrata_ su $\\mathcal{T}$!!! La classe GridSearchCV, a fine procedimento, addestra infatti il miglior modello sulle coppie input-output in argomento al metodo fit; nel nostro caso, addestra cioè su tutto $\\mathcal{D}$. Questa operazione è dovuta al fatto che la classe è stata pensata principalmente per l'uso di default con la cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo delle predizioni sul test set.\n",
    "# Nomi delle variabili:\n",
    "#\n",
    "# X_train: matrice dei volti (vettorizzati) del training set;\n",
    "# X_val: matrice dei volti (vettorizzati) del validation;\n",
    "# X_test: matrice dei volti (vettorizzati) del test set\n",
    "#\n",
    "# y_pred_train: vettore delle predizioni sul training;\n",
    "# y_true_train: vettore dei target \"veri\" del training set;\n",
    "# y_pred_val: vettore delle predizioni sul validation;\n",
    "# y_true_val: vettore dei target \"veri\" del validation set;\n",
    "#\n",
    "# y_pred: vettore delle predizioni sul test;\n",
    "# y_true: vettore dei target \"veri\" del test set\n",
    "\n",
    "# X_train = ... <-- COMPLETARE!\n",
    "# X_val = ... <-- COMPLETARE!\n",
    "# X_test = ... <-- COMPLETARE!\n",
    "\n",
    "# y_true_train = ... <-- COMPLETARE!\n",
    "# y_true_val = ... <-- COMPLETARE!\n",
    "# y_true = ... <-- COMPLETARE!\n",
    "\n",
    "# Ri-addestramento della miglior SVM (NECESSARIO A CAUSA DI REFIT)\n",
    "# svm_gs.best_estimator_.fit(...)  # <------------------------------------- COMPLETARE, IMPORTANTE!\n",
    "\n",
    "# y_pred_train = svm_gs.best_estimator_.predict(...) <-- COMPLETARE!\n",
    "# y_pred_val = svm_gs.best_estimator_.predict(...) <-- COMPLETARE!\n",
    "# y_pred = svm_gs.best_estimator_.predict(...) <-- COMPLETARE!\n",
    "\n",
    "# acc_train = svm_gs.best_estimator_.score(...) <-- COMPLETARE!\n",
    "# prec_train = precision_score(..., average='weighted') <-- COMPLETARE!\n",
    "# rec_train = recall_score(..., average='weighted') <-- COMPLETARE!\n",
    "# f1_train = f1_score(..., average='weighted') <-- COMPLETARE!\n",
    "\n",
    "# acc_val = svm_gs.best_estimator_.score(...) <-- COMPLETARE!\n",
    "# prec_val = precision_score(..., average='weighted') <-- COMPLETARE!\n",
    "# rec_val = recall_score(..., average='weighted') <-- COMPLETARE!\n",
    "# f1_val = f1_score(..., average='weighted') <-- COMPLETARE!\n",
    "\n",
    "# acc = svm_gs.best_estimator_.score(...) <-- COMPLETARE!\n",
    "# prec = precision_score(..., average='weighted') <-- COMPLETARE!\n",
    "# rec = recall_score(..., average='weighted') <-- COMPLETARE!\n",
    "# f1 = f1_score(..., average='weighted') <-- COMPLETARE!\n",
    "\n",
    "df_perf = pd.DataFrame({'Accuracy': [acc_train, acc_val, acc], \n",
    "                        'Precision': [prec_train, prec_val, prec], \n",
    "                        'Recall': [rec_train, rec_val, rec],\n",
    "                        'F1': [f1_train, f1_val, f1]\n",
    "                       },\n",
    "                      index=['training', 'validation', 'test'])\n",
    "\n",
    "# cmat = confusion_matrix(..., labels=svm_gs.best_estimator_.classes_) <-- COMPLETARE!\n",
    "# cmat_norm_true = confusion_matrix(..., labels=svm_gs.best_estimator_.classes_, normalize=...)  # recall_confusion_matrix <-- COMPLETARE!\n",
    "# cmat_norm_pred = confusion_matrix(..., labels=svm_gs.best_estimator_.classes_, normalize=...)  # precision_confusion_matrix <-- COMPLETARE!\n",
    "\n",
    "df_cmat = pd.DataFrame(cmat, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_true = pd.DataFrame(cmat_norm_true, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_pred = pd.DataFrame(cmat_norm_pred, columns=face_tnames, index=face_tnames)\n",
    "\n",
    "display(df_perf)\n",
    "display(df_cmat)\n",
    "display(df_cmat_norm_true)\n",
    "display(df_cmat_norm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-reference",
   "metadata": {},
   "source": [
    "## Alcuni Esempi Visivi\n",
    "\n",
    "Mostriamo visivamente come viene fatta la classificazione multi-classe. \n",
    "\n",
    "**RICORDA:** la classe SVC non addestra un'unica SVM (se $m>2$); in realtà esegue il metodo \"One-VS-One\", addestrando quindi $\\begin{pmatrix}m \\\\ 2\\end{pmatrix}$ SVM per ogni coppia di classi. La predizione su $\\boldsymbol{x}$ restituisce quindi la classe $\\widehat{y}$ se questa è la classe predetta in maggioranza tra tutte le SVM \"interne\".\n",
    "\n",
    "**In Scikit-Learn:** con le opzioni di default utilizzate nel codice sopra, il metodo \"decision_function\" di una SVC multiclasse è impostato su \"ovr\" (vedi sezione sopra su classificazione non binaria per SVM). Per ogni $\\boldsymbol{x}$, il metodo in questione restituisce quindi un vettore di $m$ elementi, dove l'$i$-esimo elemento ha valore maggiore se l'$i$-esima classe è quella predetta maggiormente dalle SVM \"interne\". \n",
    "\n",
    "**ATTENZIONE:** questi valori non sono né percentuali né interi, sono il risultato di una trasformazione eseguita da scikit-learn per \"elaborare\" più facilmente il risultato delle predizioni fatte dalle SVM \"interne\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviazione nomi per etichette in barplot\n",
    "face_tnames_short = []\n",
    "for name in face_tnames:\n",
    "    name_split = name.split(' ')\n",
    "    nm = ''\n",
    "    for word in name_split:\n",
    "        nm = nm + word[0]\n",
    "    face_tnames_short.append(nm)\n",
    "\n",
    "# Selezione di \"n_randsamples\" volti random dal dataset\n",
    "\n",
    "n_randsamples = 25\n",
    "ind_test_rand = np.random.choice(len(ind_test), n_randsamples, replace=False)\n",
    "ind_test_rand = ind_test[ind_test_rand]\n",
    "\n",
    "# Matrice delle n_randsamples volti scelti (una riga, un volto)\n",
    "rand_faces = face_data[ind_test_rand, :]\n",
    "\n",
    "# Decision Function per i volti random:\n",
    "rand_faces_decision = svm_gs.best_estimator_.decision_function(rand_faces)\n",
    "y_pred_rand_faces = svm_gs.best_estimator_.predict(rand_faces)\n",
    "\n",
    "for i in range(n_randsamples):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    ii = ind_test_rand[i]\n",
    "    face_ii = face_images[ii]\n",
    "    \n",
    "    axs[0].imshow(face_ii, cmap=plt.cm.gray)\n",
    "    axs[0].set_title('Volto {} ({})'.format(ii, face_tnames[face_targets[ii]]))\n",
    "    \n",
    "    axs[1].bar(np.arange(len(face_tnames)),\n",
    "               rand_faces_decision[i, :]\n",
    "              )\n",
    "    axs[1].grid()\n",
    "    axs[1].set_xticks(np.arange(len(face_tnames)))\n",
    "    axs[1].set_xticklabels(face_tnames_short,\n",
    "                           rotation=15,\n",
    "                           fontsize=12\n",
    "                          )\n",
    "    axs[1].set_title('Predizione: {}'.format(face_tnames[y_pred_rand_faces[i]]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-canberra",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
